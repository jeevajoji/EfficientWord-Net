{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU soundfile librosa pyaudio audiomentations pydub matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no useeeeeeeeeeeeeeeeeee\n",
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "\n",
    "output_dir = r\"C:\\Users\\jeeva\\Videos\\Efficient_word_net/test\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "file_urls = [\n",
    "    \"https://huggingface.co/datasets/MLCommons/ml_spoken_words/resolve/main/data/wav/en/test/audio/0.tar.gz\",\n",
    "    \"https://huggingface.co/datasets/MLCommons/ml_spoken_words/resolve/main/data/wav/en/test/audio/1.tar.gz\",\n",
    "    \"https://huggingface.co/datasets/MLCommons/ml_spoken_words/resolve/main/data/wav/en/test/audio/2.tar.gz\",\n",
    "    \"https://huggingface.co/datasets/MLCommons/ml_spoken_words/resolve/main/data/wav/en/test/audio/3.tar.gz\",\n",
    "    \"https://huggingface.co/datasets/MLCommons/ml_spoken_words/resolve/main/data/wav/en/test/audio/4.tar.gz\",\n",
    "    \"https://huggingface.co/datasets/MLCommons/ml_spoken_words/resolve/main/data/wav/en/test/audio/5.tar.gz\",\n",
    "    \"https://huggingface.co/datasets/MLCommons/ml_spoken_words/resolve/main/data/wav/en/test/audio/6.tar.gz\",\n",
    "    \"https://huggingface.co/datasets/MLCommons/ml_spoken_words/resolve/main/data/wav/en/test/audio/7.tar.gz\",\n",
    "    \"https://huggingface.co/datasets/MLCommons/ml_spoken_words/resolve/main/data/wav/en/test/audio/8.tar.gz\",\n",
    "    \"https://huggingface.co/datasets/MLCommons/ml_spoken_words/resolve/main/data/wav/en/test/audio/9.tar.gz\",\n",
    "    \"https://huggingface.co/datasets/MLCommons/ml_spoken_words/resolve/main/data/wav/en/test/audio/10.tar.gz\",\n",
    "    \"https://huggingface.co/datasets/MLCommons/ml_spoken_words/resolve/main/data/wav/en/test/audio/11.tar.gz\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Function to download a file\n",
    "def download_file(url, output_dir):\n",
    "    local_filename = os.path.join(output_dir, url.split(\"/\")[-1])\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    return local_filename\n",
    "\n",
    "# Function to extract .tar.gz files\n",
    "def extract_tar_gz(file_path, extract_dir):\n",
    "    with tarfile.open(file_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=extract_dir)\n",
    "        print(f\"Extracted: {file_path}\")\n",
    "    # Check for .tar file and extract it\n",
    "    tar_file = file_path.replace('.tar.gz', '.tar')\n",
    "    if os.path.exists(tar_file):\n",
    "        extract_tar(tar_file, extract_dir)\n",
    "\n",
    "# Function to extract .tar files\n",
    "def extract_tar(file_path, extract_dir):\n",
    "    with tarfile.open(file_path, \"r:\") as tar:\n",
    "        tar.extractall(path=extract_dir)\n",
    "        print(f\"Extracted: {file_path}\")\n",
    "\n",
    "# Download and extract all files\n",
    "for url in file_urls:\n",
    "    # Download the file\n",
    "    downloaded_file = download_file(url, output_dir)\n",
    "    print(f\"Downloaded: {downloaded_file}\")\n",
    "    \n",
    "    # Extract the file\n",
    "    extract_tar_gz(downloaded_file, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display # for waveplots, spectograms, etc\n",
    "import IPython.display as ipd # for playing files within pythonimport numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "AUDIO_WINDOW = 1.0 #seconds\n",
    "AUDIO_LENGTH = int(AUDIO_WINDOW * SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomCrop(x:np.array,length=AUDIO_LENGTH)->np.array :\n",
    "    assert(x.shape[0]>length)\n",
    "    frontBits = random.randint(0,x.shape[0]-length) \n",
    "    return x[frontBits:frontBits+length]\n",
    "\n",
    "def addPadding(x:np.array,length=AUDIO_LENGTH)->np.array :\n",
    "    assert(x.shape[0]<length)\n",
    "    bitCountToBeAdded = length - x.shape[0]\n",
    "    frontBits = random.randint(0,bitCountToBeAdded)\n",
    "    #print(frontBits, bitCountToBeAdded-frontBits)\n",
    "    new_x = np.append(np.zeros(frontBits),x)\n",
    "    new_x = np.append(new_x,np.zeros(bitCountToBeAdded-frontBits))\n",
    "    assert new_x.shape[0] == length, f\"Error: Padded audio shape is {new_x.shape}, expected {length}\"\n",
    "    return new_x\n",
    "\n",
    "def removeExistingPadding(x:np.array)->np.array:\n",
    "    lastZeroBitBeforeAudio = 0 \n",
    "    firstZeroBitAfterAudio = len(x)\n",
    "    for i in range(len(x)):\n",
    "        if x[i]==0:\n",
    "            lastZeroBitBeforeAudio = i\n",
    "        else:\n",
    "            break\n",
    "    for i in range(len(x)-1,1,-1):\n",
    "        if x[i]==0:\n",
    "            firstZeroBitAfterAudio = i\n",
    "        else:\n",
    "            break\n",
    "    return x[lastZeroBitBeforeAudio:firstZeroBitAfterAudio]\n",
    "\n",
    "\n",
    "def fixPaddingIssues(x:np.array,length=AUDIO_LENGTH)-> np.array:\n",
    "    x = removeExistingPadding(x)\n",
    "    #x = randomAugumentation(x)\n",
    "    #print(\"Preprocessing Shape\",x.shape[0])\n",
    "    if(x.shape[0]>length):\n",
    "        return randomCrop(x,length=length)\n",
    "    elif(x.shape[0]<length):\n",
    "        return addPadding(x,length=length)\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def addNoise(x:np.array,noise:np.array,noise_factor = 0.4)-> np.array:\n",
    "    assert(x.shape[0]==noise.shape[0])\n",
    "    out = (1-noise_factor)*x/x.max() + noise_factor*(noise/noise.max())\n",
    "    return out/out.max()\n",
    "\n",
    "def splitNoiseFileToChunks(filename: str, target_folder: str, count=100, sr=16000):\n",
    "    noiseAudio, _ = librosa.load(filename, sr=sr)\n",
    "    if len(noiseAudio) <= AUDIO_LENGTH:\n",
    "        print(f\"Warning: Audio file {filename} is shorter than {AUDIO_LENGTH / SAMPLE_RATE} seconds. Skipping this file.\")\n",
    "        return  # Skip this file if it's too short\n",
    "    \n",
    "    for i in range(count):\n",
    "        noiseAudioCrop = randomCrop(noiseAudio)  # This should now always work\n",
    "        outFilePath = target_folder + \"/\" + (f\"{'.'.join(filename.split('.')[:-1])}_{i}.wav\").split(\"/\")[-1]\n",
    "        sf.write(outFilePath, noiseAudioCrop, sr, 'PCM_24')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no useeeeeeeeeeeeeeeeeeee\n",
    "from os import listdir, mkdir\n",
    "from os.path import isdir\n",
    "from os import system as run_sys_command\n",
    "import librosa\n",
    "\n",
    "# Define the source and target directories\n",
    "source_dir = \"./noise_subset\"\n",
    "target_dir = \"./NoiseChunked\"\n",
    "\n",
    "# Create the target directory\n",
    "mkdir(target_dir)\n",
    "\n",
    "# List the 10 folders (folds) in the UrbanSound8K dataset\n",
    "folds = listdir(source_dir)\n",
    "\n",
    "# Iterate over each fold\n",
    "for i, fold in enumerate(folds):\n",
    "    source_path = f\"{source_dir}/{fold}\"  # Path to the current fold\n",
    "    target_path = f\"{target_dir}/{fold}\"  # Path to the target folder for chunks\n",
    "    \n",
    "    print(f\"{len(folds) - i} folds left to process.\")\n",
    "    \n",
    "    if isdir(source_path):  # Check if the fold is a directory\n",
    "        mkdir(target_path)  # Create a target folder for the current fold\n",
    "        audioFiles = listdir(source_path)  # List all audio files in the current fold\n",
    "\n",
    "        # Iterate over each audio file in the current fold\n",
    "        for j, audioFile in enumerate(audioFiles):\n",
    "            print(f\"Processing {j + 1} out of {len(audioFiles)} audio files in {fold}.\")\n",
    "            srcFilePath = f\"{source_path}/{audioFile}\"  \n",
    "            splitNoiseFileToChunks(srcFilePath, target_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from audiomentations import Compose, TimeStretch, PitchShift, Shift\n",
    "\n",
    "augmentation_pipeline = Compose([\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "    Shift(min_shift=-0.5, max_shift=0.5, p=0.5),  # Corrected Shift\n",
    "])\n",
    "\n",
    "#def randomAugumentation(x,sr=SAMPLE_RATE):\n",
    "#    choice1 = random.randint(0,1)\n",
    "#    choice2 = random.randint(0,1)\n",
    "#    return Augumentations[choice1](\n",
    "#        Augumentations[2+choice2](\n",
    "#            x,\n",
    "#            sample_rate=sr\n",
    "#        ),\n",
    "#        sample_rate=sr\n",
    "#    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "LOG_MEL_MEAN = 1.4\n",
    "LOG_MEL_STD = 1.184\n",
    "\n",
    "# DFT matrix\n",
    "def _dft_matrix(dft_length):\n",
    "    omega = (0 + 1j) * 2.0 * np.pi / float(dft_length)\n",
    "    return np.exp(omega * np.outer(np.arange(dft_length), np.arange(dft_length)))\n",
    "\n",
    "# Naive real-input Fourier Transform by matmul\n",
    "def _naive_rdft(signal_tensor, fft_length, padding='center'):\n",
    "    complex_dft_matrix_kept_values = _dft_matrix(fft_length)[:(fft_length // 2 + 1), :].transpose()\n",
    "    real_dft_tensor = tf.constant(np.real(complex_dft_matrix_kept_values).astype(np.float32), name='real_dft_matrix')\n",
    "    imag_dft_tensor = tf.constant(np.imag(complex_dft_matrix_kept_values).astype(np.float32), name='imaginary_dft_matrix')\n",
    "    \n",
    "    signal_frame_length = signal_tensor.shape[-1]\n",
    "    half_pad = (fft_length - signal_frame_length) // 2\n",
    "\n",
    "    if padding == 'center':  # Center-padding\n",
    "        pad_values = tf.concat([\n",
    "            tf.zeros([tf.rank(signal_tensor) - 1, 2], tf.int32),\n",
    "            [[half_pad, fft_length - signal_frame_length - half_pad]]\n",
    "        ], axis=0)\n",
    "    elif padding == 'right':  # Right-padding\n",
    "        pad_values = tf.concat([\n",
    "            tf.zeros([tf.rank(signal_tensor) - 1, 2], tf.int32),\n",
    "            [[0, fft_length - signal_frame_length]]\n",
    "        ], axis=0)\n",
    "\n",
    "    padded_signal = tf.pad(signal_tensor, pad_values)\n",
    "\n",
    "    # Check shapes after padding\n",
    "    #print(f\"Padded signal shape: {padded_signal.shape}\")\n",
    "\n",
    "    result_real_part = tf.matmul(padded_signal, real_dft_tensor)\n",
    "    result_imag_part = tf.matmul(padded_signal, imag_dft_tensor)\n",
    "\n",
    "    #print(f\"Real part shape: {result_real_part.shape}, Imaginary part shape: {result_imag_part.shape}\")\n",
    "    \n",
    "    return result_real_part, result_imag_part\n",
    "\n",
    "def _fixed_frame(signal, frame_length, frame_step, first_axis=False):\n",
    "    signal_shape = signal.shape.as_list()\n",
    "\n",
    "    length_samples = signal_shape[0] if first_axis else signal_shape[-1]\n",
    "    \n",
    "    if length_samples <= 0:\n",
    "        raise ValueError('fixed framing requires predefined constant signal length')\n",
    "\n",
    "    num_frames = max(0, 1 + (length_samples - frame_length) // frame_step)\n",
    "\n",
    "    outer_dimensions = signal_shape[:-1]\n",
    "    result_shape = outer_dimensions + [num_frames, frame_length]\n",
    "\n",
    "    subframe_length = math.gcd(frame_length, frame_step)\n",
    "    subframes_per_frame = frame_length // subframe_length\n",
    "    subframes_per_hop = frame_step // subframe_length\n",
    "    num_subframes = length_samples // subframe_length\n",
    "\n",
    "    trimmed_input_size = outer_dimensions + [num_subframes * subframe_length]\n",
    "    subframe_shape = outer_dimensions + [num_subframes, subframe_length]\n",
    "    \n",
    "    subframes = tf.reshape(\n",
    "        tf.slice(\n",
    "            signal,\n",
    "            begin=np.zeros(len(signal_shape), np.int32),\n",
    "            size=trimmed_input_size), subframe_shape)\n",
    "\n",
    "    frame_selector = np.reshape(np.arange(num_frames) * subframes_per_hop, [num_frames, 1])\n",
    "    subframe_selector = np.reshape(np.arange(subframes_per_frame), [1, subframes_per_frame])\n",
    "\n",
    "    selector = frame_selector + subframe_selector\n",
    "    frames = tf.reshape(tf.gather(subframes, selector.astype(np.int32), axis=-2), result_shape)\n",
    "\n",
    "    #print(f\"Framed signal shape: {frames.shape}\")\n",
    "    return frames\n",
    "\n",
    "def _stft_tflite(signal, frame_length, frame_step, fft_length):\n",
    "    window = tf.reshape(\n",
    "        tf.constant(\n",
    "            (0.5 - 0.5 * np.cos(2 * np.pi * np.arange(0, 1.0, 1.0 / frame_length))\n",
    "            ).astype(np.float32),\n",
    "            name='window'), [1, frame_length])\n",
    "    \n",
    "    framed_signal = _fixed_frame(signal, frame_length, frame_step, first_axis=False)\n",
    "    framed_signal *= window\n",
    "\n",
    "    real_spectrogram, imag_spectrogram = _naive_rdft(framed_signal, fft_length)\n",
    "\n",
    "    #print(f\"STFT real part shape: {real_spectrogram.shape}, STFT imag part shape: {imag_spectrogram.shape}\")\n",
    "    \n",
    "    return real_spectrogram, imag_spectrogram\n",
    "\n",
    "def _stft_magnitude_tflite(signals, frame_length, frame_step, fft_length):\n",
    "    real_stft, imag_stft = _stft_tflite(signals, frame_length, frame_step, fft_length)\n",
    "    stft_magnitude = tf.sqrt(tf.add(real_stft * real_stft, imag_stft * imag_stft), name='magnitude_spectrogram')\n",
    "\n",
    "    #print(f\"STFT magnitude shape: {stft_magnitude.shape}\")\n",
    "    \n",
    "    return stft_magnitude\n",
    "\n",
    "def build_mel_calculation_graph(waveform):\n",
    "    with tf.name_scope('log_mel_features'):\n",
    "        window_length_samples = 400\n",
    "        hop_length_samples = 160\n",
    "        MEL_BANDS = 64\n",
    "        SAMPLE_RATE = 16000\n",
    "        LOG_OFFSET = 0.001\n",
    "        MEL_MIN_HZ = 50\n",
    "        MEL_MAX_HZ = 8000\n",
    "        fft_length = 2 ** int(np.ceil(np.log(window_length_samples) / np.log(2.0)))\n",
    "        num_spectrogram_bins = fft_length // 2 + 1\n",
    "\n",
    "        magnitude_spectrogram = _stft_magnitude_tflite(\n",
    "            signals=waveform, \n",
    "            frame_length=window_length_samples, \n",
    "            frame_step=hop_length_samples, \n",
    "            fft_length=fft_length)\n",
    "\n",
    "        linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "            num_mel_bins=MEL_BANDS,\n",
    "            num_spectrogram_bins=num_spectrogram_bins,\n",
    "            sample_rate=SAMPLE_RATE,\n",
    "            lower_edge_hertz=MEL_MIN_HZ,\n",
    "            upper_edge_hertz=MEL_MAX_HZ)\n",
    "\n",
    "        mel_spectrogram = tf.matmul(magnitude_spectrogram, linear_to_mel_weight_matrix)\n",
    "        log_mel_spectrogram = (tf.math.log(\n",
    "            mel_spectrogram + LOG_OFFSET) - LOG_MEL_MEAN) / LOG_MEL_STD\n",
    "\n",
    "        #print(f\"Log mel spectrogram shape: {log_mel_spectrogram.shape}\")\n",
    "        return np.expand_dims(log_mel_spectrogram, axis=-1)\n",
    "\n",
    "# Test the build function\n",
    "t = np.random.random(16000)\n",
    "t = tf.constant(t, dtype=tf.float32)\n",
    "t = build_mel_calculation_graph(t)\n",
    "\n",
    "print(t.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "\n",
    "# Helper function to omit hidden files\n",
    "def omitHiddenFiles(inpArray):\n",
    "    return [x for x in inpArray if '.' != x[0]]\n",
    "\n",
    "class CustomDataGen(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, chunkedNoisePath, datasetPath,\n",
    "                 batch_size=16,\n",
    "                 training_generator=True,\n",
    "                 max_noise_factor=0.2,\n",
    "                 min_noise_factor=0.05,\n",
    "                 sampling_rate=16000,\n",
    "                 spectrogram=True,\n",
    "                 shuffle=True,\n",
    "                 print_words=False):\n",
    "    \n",
    "        self.batch_size = batch_size\n",
    "        self.spectrogram = spectrogram\n",
    "        self.shuffle = shuffle\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.chunkedNoisePath = chunkedNoisePath\n",
    "        self.typesOfNoise = omitHiddenFiles(os.listdir(self.chunkedNoisePath))\n",
    "        self.datasetPath = datasetPath\n",
    "        self.max_noise_factor = max_noise_factor\n",
    "        self.min_noise_factor = min_noise_factor\n",
    "        self.wordsInDataset = omitHiddenFiles(os.listdir(datasetPath))\n",
    "        self.print_words = print_words\n",
    "        countOfWords = len(self.wordsInDataset)\n",
    "\n",
    "        if training_generator:\n",
    "            self.wordsInDataset = self.wordsInDataset[:int(0.9 * countOfWords)]\n",
    "            print(\"Train size: \", len(self.wordsInDataset))\n",
    "        else:\n",
    "            self.wordsInDataset = self.wordsInDataset[int(0.9 * countOfWords):]\n",
    "            print(\"Test size: \", len(self.wordsInDataset))\n",
    "\n",
    "        # Shuffle the dataset\n",
    "        self.wordsInDataset = random.sample(self.wordsInDataset, len(self.wordsInDataset))\n",
    "\n",
    "        self.n = 2 * len(self.wordsInDataset)  # Positive and negative pairs\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Randomizing word order on epoch end\n",
    "        self.wordsInDataset = random.sample(self.wordsInDataset, len(self.wordsInDataset))\n",
    "    \n",
    "    def giveJoinedAudio(self, word1: str, word2: str):\n",
    "        if self.print_words:\n",
    "            print(word1, word2)\n",
    "        if word1 == word2:\n",
    "            sample1, sample2 = random.sample(omitHiddenFiles(os.listdir(self.datasetPath + \"/\" + word1)), 2)\n",
    "        else:\n",
    "            sample1 = random.choice(omitHiddenFiles(os.listdir(self.datasetPath + \"/\" + word1)))\n",
    "            sample2 = random.choice(omitHiddenFiles(os.listdir(self.datasetPath + \"/\" + word2)))\n",
    "\n",
    "        voiceVector1, _ = librosa.load(self.datasetPath + \"/\" + word1 + \"/\" + sample1, sr=self.sampling_rate)\n",
    "        voiceVector2, _ = librosa.load(self.datasetPath + \"/\" + word2 + \"/\" + sample2, sr=self.sampling_rate)\n",
    "\n",
    "        # Padding\n",
    "        voiceVector1 = fixPaddingIssues(voiceVector1)\n",
    "        voiceVector2 = fixPaddingIssues(voiceVector2)\n",
    "\n",
    "        # Noise generation\n",
    "        randomNoiseType1, randomNoiseType2 = random.sample(self.typesOfNoise, 2)\n",
    "        randomNoise1 = random.choice(omitHiddenFiles(os.listdir(self.chunkedNoisePath + \"/\" + randomNoiseType1 + \"/\")))\n",
    "        randomNoise2 = random.choice(omitHiddenFiles(os.listdir(self.chunkedNoisePath + \"/\" + randomNoiseType2 + \"/\")))\n",
    "\n",
    "        noiseVector1, _ = librosa.load(self.chunkedNoisePath + \"/\" + randomNoiseType1 + \"/\" + randomNoise1, sr=self.sampling_rate)\n",
    "        noiseVector2, _ = librosa.load(self.chunkedNoisePath + \"/\" + randomNoiseType2 + \"/\" + randomNoise2, sr=self.sampling_rate)\n",
    "\n",
    "        # Noise factors\n",
    "        randomNoiseFactor1 = random.uniform(self.min_noise_factor, self.max_noise_factor)\n",
    "        randomNoiseFactor2 = random.uniform(self.min_noise_factor, self.max_noise_factor)\n",
    "\n",
    "        voice_with_noise1 = addNoise(voiceVector1, noiseVector1, randomNoiseFactor1)\n",
    "        voice_with_noise2 = addNoise(voiceVector2, noiseVector2, randomNoiseFactor2)\n",
    "\n",
    "        if self.spectrogram:\n",
    "            voice_with_noise_spectrogram1 = build_mel_calculation_graph(tf.constant(voice_with_noise1, dtype=tf.float32))\n",
    "            voice_with_noise_spectrogram2 = build_mel_calculation_graph(tf.constant(voice_with_noise2, dtype=tf.float32))\n",
    "\n",
    "            # Check if spectrograms are valid\n",
    "            if voice_with_noise_spectrogram1.shape[1] > 0 and voice_with_noise_spectrogram2.shape[1] > 0:\n",
    "                return np.array([voice_with_noise_spectrogram1, voice_with_noise_spectrogram2])\n",
    "            else:\n",
    "                raise ValueError(\"Generated spectrogram is empty!\")\n",
    "\n",
    "        return np.array([voice_with_noise1, voice_with_noise2])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        firstPairIndex = index * self.batch_size\n",
    "        X1, X2, Y = [], [], []\n",
    "\n",
    "        for i in range(firstPairIndex, firstPairIndex + self.batch_size):\n",
    "            x_data = self.giveJoinedAudio(\n",
    "                self.wordsInDataset[i // 2],\n",
    "                self.wordsInDataset[(i // 2 + i % 2) % (self.n // 2)]\n",
    "                )\n",
    "            X1.append(x_data[0])  # First audio (X1)\n",
    "            X2.append(x_data[1])  # Second audio (X2)\n",
    "            Y.append(1.0 if i % 2 == 0 else 0.0)  # Labels\n",
    "\n",
    "        # Stack arrays for output\n",
    "        X1 = np.array(X1)\n",
    "        X2 = np.array(X2)\n",
    "        Y = np.array(Y)\n",
    "\n",
    "    \n",
    "        # Return a tuple where the first element is a list of two inputs\n",
    "        return (X1, X2), Y  # Return two inputs and the labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkedNoisePath = r\"C:\\Users\\jeeva\\Videos\\Efficient_word_net\\NoiseChunked\"\n",
    "datasetPath = r\"C:\\Users\\jeeva\\Videos\\Efficient_word_net\\test\"\n",
    "\n",
    "giveSpectrogram = False\n",
    "\n",
    "generator = CustomDataGen(\n",
    "    chunkedNoisePath,\n",
    "    datasetPath,\n",
    "    batch_size = 2,\n",
    "    max_noise_factor=0.18,\n",
    "    min_noise_factor=0.12, \n",
    "    spectrogram = giveSpectrogram,\n",
    "    print_words=True\n",
    "    )\n",
    "\n",
    "X1,Y1 = generator[64]\n",
    "data1 = X1[0][0]\n",
    "data2 = X1[0][1]\n",
    "\n",
    "print(data1.shape)\n",
    "\n",
    "ipd.Audio(np.concatenate((data1,data2)),rate=SAMPLE_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax = plt.subplots(1,2)\n",
    "ax[0].imshow(\n",
    "    build_mel_calculation_graph(tf.constant(data1,dtype=tf.float32))[...,0]\n",
    "    )\n",
    "ax[1].imshow(\n",
    "    build_mel_calculation_graph(tf.constant(data2,dtype=tf.float32))[...,0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inpShape=(98,64,1)\n",
    "#model = tf.keras.applications.EfficientNetB0(include_top=False,weights=None,input_shape=inpShape)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def l2_reg_output(x):\n",
    "#    return tf.keras.backend.l2_normalize(x, axis=1)\n",
    "\n",
    "#outputLayer = model.get_layer('conv4_block6_out').output  \n",
    "\n",
    "#x = tf.keras.layers.Conv2D(32, 3, padding=\"same\", name=\"extra_conv1\", kernel_regularizer=\"l2\")(outputLayer)\n",
    "#x = tf.keras.layers.BatchNormalization()(x)\n",
    "#x = tf.keras.layers.MaxPooling2D()(x)\n",
    "#x = tf.keras.layers.Conv2D(64, 3, padding=\"same\", name=\"extra_conv2\", kernel_regularizer=\"l2\")(x)\n",
    "#x = tf.keras.layers.AveragePooling2D(pool_size=(x.shape[1]//2, x.shape[2]//2), strides=None, padding=\"valid\")(x)\n",
    "#x = tf.keras.layers.Flatten()(x)\n",
    "#x = tf.keras.layers.Dense(128, kernel_regularizer=\"l2\")(x)\n",
    "\n",
    "#x = tf.keras.layers.Lambda(l2_reg_output)(x)\n",
    "\n",
    "#baseNetwork = tf.keras.Model(inputs=model.input, outputs=x, name=\"basemodel\")\n",
    "#baseNetwork.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true,y_pred):\n",
    "    match_loss =  y_true * -2.0 * tf.math.log( 1 - y_pred/2 ) \n",
    "    mismatch_loss =  tf.maximum((1 - y_true) * ( -tf.math.log(y_pred/0.2) ) ,0)\n",
    "\n",
    "    return tf.reduce_mean( match_loss + mismatch_loss )\n",
    "\n",
    "def accuracy(y_true ,y_pred):\n",
    "    threshold_check = tf.cast(tf.less_equal(y_pred,0.2),dtype=tf.float32)\n",
    "    return tf.keras.metrics.binary_accuracy(y_true,threshold_check)\n",
    "\n",
    "##Test code\n",
    "t = tf.constant((1.0,1.0,1.0,1.0,1.0,1.0))\n",
    "q = t - 0\n",
    "\n",
    "print(triplet_loss(t,q))\n",
    "print(accuracy(t,q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet - 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inpShape = (98, 64, 1)  # Original input shape (1 channel)\n",
    "\n",
    "# Define a lambda layer to replicate the single channel 3 times\n",
    "input_a = tf.keras.layers.Input(shape=inpShape)\n",
    "input_b = tf.keras.layers.Input(shape=inpShape)\n",
    "\n",
    "# Replicate the input to create 3 channels (convert 1 channel to 3)\n",
    "replicate_a = tf.keras.layers.Lambda(lambda x: tf.keras.backend.repeat_elements(x, 3, axis=-1))(input_a)\n",
    "replicate_b = tf.keras.layers.Lambda(lambda x: tf.keras.backend.repeat_elements(x, 3, axis=-1))(input_b)\n",
    "\n",
    "# Load ResNet50 and use the replicated 3-channel inputs\n",
    "base_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=(98, 64, 3))\n",
    "\n",
    "for layer in base_model.layers[:-20]:  \n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "# Add custom layers on top of ResNet50\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(128, kernel_regularizer='l2')(x)\n",
    "x = tf.keras.layers.Lambda(lambda x: tf.keras.backend.l2_normalize(x, axis=1))(x)\n",
    "\n",
    "# Define base network for the Siamese model\n",
    "base_network = tf.keras.Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Apply the base network to both inputs\n",
    "embedding_a = base_network(replicate_a)\n",
    "embedding_b = base_network(replicate_b)\n",
    "\n",
    "# Compute the Euclidean distance between the two embeddings\n",
    "distance = tf.keras.layers.Lambda(lambda embeddings: tf.keras.backend.sqrt(tf.keras.backend.sum(tf.keras.backend.square(embeddings[0] - embeddings[1]), axis=-1)))(\n",
    "    [embedding_a, embedding_b]\n",
    ")\n",
    "def triplet_loss(y_true,y_pred):\n",
    "    match_loss =  y_true * -2.0 * tf.math.log( 1 - y_pred/2 ) \n",
    "    mismatch_loss =  tf.maximum((1 - y_true) * ( -tf.math.log(y_pred/0.2) ) ,0)\n",
    "    return tf.reduce_mean( match_loss + mismatch_loss )\n",
    "\n",
    "def accuracy(y_true ,y_pred):\n",
    "    threshold_check = tf.cast(tf.less_equal(y_pred,0.2),dtype=tf.float32)\n",
    "    return tf.keras.metrics.binary_accuracy(y_true,threshold_check)\n",
    "# Create the Siamese model\n",
    "siamese_model = tf.keras.Model(inputs=[input_a, input_b], outputs=distance)\n",
    "\n",
    "# Compile the model\n",
    "siamese_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=triplet_loss, metrics=[accuracy])\n",
    "\n",
    "# Callbacks for training\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=r\"C:\\Users\\jeeva\\Videos\\Efficient_word_net\\Checkpoints\\model-{epoch:02d}-loss_{loss:.3f}_val_acc{val_accuracy:.3f}.keras\", \n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=False\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='loss', factor=0.1, patience=2, min_lr=1e-6\n",
    "    ),\n",
    "]\n",
    "# Train the model with the adjusted generators\n",
    "history = siamese_model.fit(\n",
    "    training_generator,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator,\n",
    "    callbacks=my_callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
