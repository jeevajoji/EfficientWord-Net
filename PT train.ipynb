{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from audiomentations import Compose, TimeStretch, PitchShift, Shift\n",
    "import torchaudio.functional as TF\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# --- Global Settings ---\n",
    "SAMPLE_RATE = 16000\n",
    "AUDIO_WINDOW = 1.0  # seconds\n",
    "AUDIO_LENGTH = int(AUDIO_WINDOW * SAMPLE_RATE)\n",
    "LOG_MEL_MEAN = 1.4\n",
    "LOG_MEL_STD = 1.184\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def randomCrop(x: np.array, length=AUDIO_LENGTH) -> np.array:\n",
    "    assert(x.shape[0] > length)\n",
    "    frontBits = random.randint(0, x.shape[0] - length)\n",
    "    return x[frontBits:frontBits + length]\n",
    "\n",
    "def addPadding(x: np.array, length=AUDIO_LENGTH) -> np.array:\n",
    "    assert(x.shape[0] < length)\n",
    "    bitCountToBeAdded = length - x.shape[0]\n",
    "    frontBits = random.randint(0, bitCountToBeAdded)\n",
    "    new_x = np.append(np.zeros(frontBits), x)\n",
    "    new_x = np.append(new_x, np.zeros(bitCountToBeAdded - frontBits))\n",
    "    assert new_x.shape[0] == length, f\"Error: Padded audio shape is {new_x.shape}, expected {length}\"\n",
    "    return new_x\n",
    "\n",
    "def removeExistingPadding(x: np.array) -> np.array:\n",
    "    lastZeroBitBeforeAudio = 0\n",
    "    firstZeroBitAfterAudio = len(x)\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == 0:\n",
    "            lastZeroBitBeforeAudio = i\n",
    "        else:\n",
    "            break\n",
    "    for i in range(len(x) - 1, 1, -1):\n",
    "        if x[i] == 0:\n",
    "            firstZeroBitAfterAudio = i\n",
    "        else:\n",
    "            break\n",
    "    return x[lastZeroBitBeforeAudio:firstZeroBitAfterAudio]\n",
    "\n",
    "def fixPaddingIssues(x: np.array, length=AUDIO_LENGTH) -> np.array:\n",
    "    x = removeExistingPadding(x)\n",
    "    if x.shape[0] > length:\n",
    "        return randomCrop(x, length=length)\n",
    "    elif x.shape[0] < length:\n",
    "        return addPadding(x, length=length)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def addNoise(x: np.array, noise: np.array, noise_factor=0.4) -> np.array:\n",
    "    assert(x.shape[0] == noise.shape[0])\n",
    "    out = (1 - noise_factor) * x / x.max() + noise_factor * (noise / noise.max())\n",
    "    return out / out.max()\n",
    "\n",
    "def splitNoiseFileToChunks(filename: str, target_folder: str, count=100, sr=16000):\n",
    "    noiseAudio, _ = librosa.load(filename, sr=sr)\n",
    "    if len(noiseAudio) <= AUDIO_LENGTH:\n",
    "        print(f\"Warning: Audio file {filename} is shorter than {AUDIO_LENGTH / SAMPLE_RATE} seconds. Skipping this file.\")\n",
    "        return\n",
    "\n",
    "    for i in range(count):\n",
    "        noiseAudioCrop = randomCrop(noiseAudio)\n",
    "        outFilePath = target_folder + \"/\" + (f\"{'.'.join(filename.split('.')[:-1])}_{i}.wav\").split(\"/\")[-1]\n",
    "        sf.write(outFilePath, noiseAudioCrop, sr, 'PCM_24')\n",
    "\n",
    "# --- Audio Augmentation ---\n",
    "augmentation_pipeline = Compose([\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "    Shift(min_shift=-0.5, max_shift=0.5, p=0.5),\n",
    "])\n",
    "\n",
    "# --- Mel Spectrogram Calculation (Corrected) ---\n",
    "def get_mel_spectrogram(waveform):\n",
    "    \"\"\"Calculates log-Mel spectrogram using torch.stft.\"\"\"\n",
    "    waveform = waveform.unsqueeze(1)\n",
    "\n",
    "    print(\"Waveform shape:\", waveform.shape)\n",
    "    print(\"Waveform device:\", waveform.device)\n",
    "\n",
    "    # STFT parameters\n",
    "    n_fft = 512\n",
    "    hop_length = 160\n",
    "    win_length = 512\n",
    "    window = torch.hann_window(win_length)\n",
    "\n",
    "    print(\"Window shape:\", window.shape)\n",
    "    print(\"Window device:\", window.device)\n",
    "\n",
    "    # Calculate STFT \n",
    "    stft_out = torch.stft(waveform,\n",
    "                          n_fft=n_fft,\n",
    "                          hop_length=hop_length,\n",
    "                          win_length=win_length,\n",
    "                          window=window.to(waveform.device),\n",
    "                          center=False,\n",
    "                          onesided=True,\n",
    "                          return_complex=True)\n",
    "\n",
    "    print(\"STFT output shape:\", stft_out.shape)  # <--- Print the shape \n",
    "    print(\"STFT output device:\", stft_out.device) # <--- Print the device\n",
    "\n",
    "    # Magnitude spectrogram\n",
    "    magnitude_spec = torch.abs(stft_out)\n",
    "\n",
    "\n",
    "    # Magnitude spectrogram\n",
    "    magnitude_spec = torch.abs(stft_out)\n",
    "\n",
    "    # Mel filter bank parameters\n",
    "    n_mels = 64\n",
    "    f_min = 50\n",
    "    f_max = 8000\n",
    "\n",
    "    # Create Mel filter bank\n",
    "    mel_filter_bank = TF.create_fb_matrix(\n",
    "        n_freqs=n_fft // 2 + 1,\n",
    "        f_min=f_min,\n",
    "        f_max=f_max,\n",
    "        n_mels=n_mels,\n",
    "        sample_rate=SAMPLE_RATE\n",
    "    )\n",
    "\n",
    "    # Apply filter bank\n",
    "    melspec = torch.matmul(magnitude_spec.transpose(1, 2), mel_filter_bank.to(magnitude_spec.device)).transpose(1, 2)\n",
    "\n",
    "    # Log-scaling and normalization\n",
    "    log_melspec = (torch.log(melspec + 1e-9) - LOG_MEL_MEAN) / LOG_MEL_STD\n",
    "\n",
    "    # Replicate channels for ResNet compatibility\n",
    "    log_melspec = log_melspec.repeat(1, 3, 1, 1) \n",
    "\n",
    "    return log_melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset , DataLoader\n",
    "\n",
    "class WakeWordDataset(Dataset):\n",
    "    def __init__(self, chunked_noise_path, dataset_path, transform=None,\n",
    "                 training=True, max_noise_factor=0.2, min_noise_factor=0.05,\n",
    "                 sampling_rate=16000, spectrogram=True, print_words=False):\n",
    "        super(WakeWordDataset, self).__init__()\n",
    "        self.chunked_noise_path = chunked_noise_path\n",
    "        self.dataset_path = dataset_path\n",
    "        self.transform = transform \n",
    "        self.training = training\n",
    "        self.max_noise_factor = max_noise_factor\n",
    "        self.min_noise_factor = min_noise_factor\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.spectrogram = spectrogram\n",
    "        self.print_words = print_words\n",
    "\n",
    "        self.types_of_noise = self.get_file_list(self.chunked_noise_path)\n",
    "        self.words_in_dataset = self.get_file_list(self.dataset_path)\n",
    "\n",
    "        if self.training:\n",
    "            self.words_in_dataset = self.words_in_dataset[:int(0.9 * len(self.words_in_dataset))]\n",
    "            print(\"Train size: \", len(self.words_in_dataset))\n",
    "        else:\n",
    "            self.words_in_dataset = self.words_in_dataset[int(0.9 * len(self.words_in_dataset)):]\n",
    "            print(\"Test size: \", len(self.words_in_dataset))\n",
    "\n",
    "    def get_file_list(self, path):\n",
    "        return [f for f in os.listdir(path) if not f.startswith('.')]\n",
    "\n",
    "    def give_joined_audio(self, word1, word2):\n",
    "        if self.print_words:\n",
    "            print(word1, word2)\n",
    "\n",
    "        sample1_path, sample2_path = self.get_audio_paths(word1, word2)\n",
    "        voice_vector1, _ = librosa.load(sample1_path, sr=self.sampling_rate)\n",
    "        voice_vector2, _ = librosa.load(sample2_path, sr=self.sampling_rate)\n",
    "\n",
    "        voice_vector1 = fixPaddingIssues(voice_vector1) \n",
    "        voice_vector2 = fixPaddingIssues(voice_vector2)\n",
    "\n",
    "        noise_vector1, noise_vector2 = self.load_noise()\n",
    "\n",
    "        random_noise_factor1 = random.uniform(self.min_noise_factor, self.max_noise_factor)\n",
    "        random_noise_factor2 = random.uniform(self.min_noise_factor, self.max_noise_factor)\n",
    "\n",
    "        voice_with_noise1 = addNoise(voice_vector1, noise_vector1, random_noise_factor1)\n",
    "        voice_with_noise2 = addNoise(voice_vector2, noise_vector2, random_noise_factor2)\n",
    "\n",
    "        if self.spectrogram:\n",
    "            voice_with_noise1 = torch.tensor(voice_with_noise1, dtype=torch.float32)\n",
    "            voice_with_noise2 = torch.tensor(voice_with_noise2, dtype=torch.float32)\n",
    "\n",
    "            voice_with_noise_spectrogram1 = get_mel_spectrogram(voice_with_noise1)\n",
    "            voice_with_noise_spectrogram2 = get_mel_spectrogram(voice_with_noise2)\n",
    "            return voice_with_noise_spectrogram1, voice_with_noise_spectrogram2\n",
    "        else:\n",
    "            return voice_with_noise1, voice_with_noise2\n",
    "\n",
    "    def get_audio_paths(self, word1, word2):\n",
    "        if word1 == word2:\n",
    "            sample1, sample2 = random.sample(self.get_file_list(os.path.join(self.dataset_path, word1)), 2)\n",
    "            return (os.path.join(self.dataset_path, word1, sample1),\n",
    "                    os.path.join(self.dataset_path, word2, sample2))\n",
    "        else:\n",
    "            sample1 = random.choice(self.get_file_list(os.path.join(self.dataset_path, word1)))\n",
    "            sample2 = random.choice(self.get_file_list(os.path.join(self.dataset_path, word2)))\n",
    "            return (os.path.join(self.dataset_path, word1, sample1),\n",
    "                    os.path.join(self.dataset_path, word2, sample2))\n",
    "    def load_noise(self):\n",
    "        noise_type1, noise_type2 = random.sample(self.types_of_noise, 2)\n",
    "        noise_file1 = random.choice(self.get_file_list(os.path.join(self.chunked_noise_path, noise_type1)))\n",
    "        noise_file2 = random.choice(self.get_file_list(os.path.join(self.chunked_noise_path, noise_type2)))\n",
    "\n",
    "        noise_vector1, _ = librosa.load(os.path.join(self.chunked_noise_path, noise_type1, noise_file1),\n",
    "                                        sr=self.sampling_rate)\n",
    "        noise_vector2, _ = librosa.load(os.path.join(self.chunked_noise_path, noise_type2, noise_file2),\n",
    "                                        sr=self.sampling_rate)\n",
    "        return noise_vector1, noise_vector2\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        word1 = self.words_in_dataset[index // 2] \n",
    "        word2 = self.words_in_dataset[(index // 2 + index % 2) % (len(self.words_in_dataset) // 2)]\n",
    "\n",
    "        x1, x2 = self.give_joined_audio(word1, word2)\n",
    "        \n",
    "        y = 1.0 if index % 2 == 0 else 0.0  # 1 for match, 0 for mismatch\n",
    "        return (x1, x2), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 2 * len(self.words_in_dataset) # Double for positive & negative pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseResNet, self).__init__()\n",
    "        self.resnet = torchvision.models.resnet50(pretrained=True)\n",
    "        # No need to modify conv1 \n",
    "\n",
    "        # Freeze early layers\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.resnet.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(2048, 128)\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.normalize(x, p=2, dim=1)  # L2 normalization\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(output1, output2, y_true, margin=1.0):\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2, p=2)\n",
    "    loss_contrastive = torch.mean((1 - y_true) * torch.pow(euclidean_distance, 2) +\n",
    "                                    (y_true) * torch.pow(torch.clamp(margin - euclidean_distance, min=0.0), 2))\n",
    "    return loss_contrastive\n",
    "\n",
    "def accuracy(output1, output2, y_true, threshold=0.2):\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2, p=2)\n",
    "    predictions = (euclidean_distance < threshold).float()\n",
    "    correct = (predictions == y_true).float().sum()\n",
    "    return (correct / len(y_true)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  21157\n",
      "Test size:  2351\n",
      "Waveform shape: torch.Size([16000, 1])\n",
      "Waveform device: cpu\n",
      "Window shape: torch.Size([512])\n",
      "Window device: cpu\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stft(torch.FloatTensor[16000, 1], n_fft=512, hop_length=160, win_length=512, window=torch.FloatTensor{[512]}, normalized=0, onesided=1, return_complex=1) : expected 0 < n_fft < 1, but got n_fft=512",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter() \n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m---> 22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jeeva\\Videos\\Efficient_word_net\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\jeeva\\Videos\\Efficient_word_net\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\jeeva\\Videos\\Efficient_word_net\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[84], line 85\u001b[0m, in \u001b[0;36mWakeWordDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     82\u001b[0m word1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords_in_dataset[index \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m] \n\u001b[0;32m     83\u001b[0m word2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords_in_dataset[(index \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m index \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwords_in_dataset) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)]\n\u001b[1;32m---> 85\u001b[0m x1, x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgive_joined_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m  \u001b[38;5;66;03m# 1 for match, 0 for mismatch\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (x1, x2), torch\u001b[38;5;241m.\u001b[39mtensor(y, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[1;32mIn[84], line 54\u001b[0m, in \u001b[0;36mWakeWordDataset.give_joined_audio\u001b[1;34m(self, word1, word2)\u001b[0m\n\u001b[0;32m     51\u001b[0m voice_with_noise1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(voice_with_noise1, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     52\u001b[0m voice_with_noise2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(voice_with_noise2, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 54\u001b[0m voice_with_noise_spectrogram1 \u001b[38;5;241m=\u001b[39m \u001b[43mget_mel_spectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvoice_with_noise1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m voice_with_noise_spectrogram2 \u001b[38;5;241m=\u001b[39m get_mel_spectrogram(voice_with_noise2)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m voice_with_noise_spectrogram1, voice_with_noise_spectrogram2\n",
      "Cell \u001b[1;32mIn[83], line 102\u001b[0m, in \u001b[0;36mget_mel_spectrogram\u001b[1;34m(waveform)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWindow device:\u001b[39m\u001b[38;5;124m\"\u001b[39m, window\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Calculate STFT \u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m stft_out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m                      \u001b[49m\u001b[43monesided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTFT output shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, stft_out\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# <--- Print the shape \u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTFT output device:\u001b[39m\u001b[38;5;124m\"\u001b[39m, stft_out\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;66;03m# <--- Print the device\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jeeva\\Videos\\Efficient_word_net\\.venv\\Lib\\site-packages\\torch\\functional.py:666\u001b[0m, in \u001b[0;36mstft\u001b[1;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex)\u001b[0m\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(extended_shape), [pad, pad], pad_mode)\n\u001b[0;32m    665\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39msignal_dim:])\n\u001b[1;32m--> 666\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_complex\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stft(torch.FloatTensor[16000, 1], n_fft=512, hop_length=160, win_length=512, window=torch.FloatTensor{[512]}, normalized=0, onesided=1, return_complex=1) : expected 0 < n_fft < 1, but got n_fft=512"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": # Add this line\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 10\n",
    "    LEARNING_RATE = 1e-4\n",
    "\n",
    "    chunkedNoisePath = r\"C:\\Users\\jeeva\\Videos\\Efficient_word_net\\NoiseChunked\" # Replace with your actual path\n",
    "    datasetPath = r\"C:\\Users\\jeeva\\Videos\\Efficient_word_net\\test\"        # Replace with your actual path\n",
    "\n",
    "    train_dataset = WakeWordDataset(chunkedNoisePath, datasetPath, spectrogram=True, training=True)\n",
    "    test_dataset = WakeWordDataset(chunkedNoisePath, datasetPath, spectrogram=True, training=False)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = SiameseResNet().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    writer = SummaryWriter() \n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for batch_idx, ((data1, data2), y) in enumerate(train_loader):\n",
    "            data1, data2, y = data1.to(device), data2.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output1, output2 = model(data1, data2) \n",
    "            \n",
    "            loss = triplet_loss(output1, output2, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
