{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\users\\salos\\onedrive\\desktop\\efficientword-net\\venv\\lib\\site-packages (2.4.1+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\salos\\onedrive\\desktop\\efficientword-net\\venv\\lib\\site-packages (0.19.1+cu121)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\salos\\onedrive\\desktop\\efficientword-net\\venv\\lib\\site-packages (2.4.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\salos\\onedrive\\desktop\\efficientword-net\\venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\salos\\onedrive\\desktop\\efficientword-net\\venv\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\salos\\onedrive\\desktop\\efficientword-net\\venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\salos\\onedrive\\desktop\\efficientword-net\\venv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\salos\\onedrive\\desktop\\efficientword-net\\venv\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\salos\\onedrive\\desktop\\efficientword-net\\venv\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\salos\\onedrive\\desktop\\efficientword-net\\venv\\lib\\site-packages (from torchvision) (1.26.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\salos\\onedrive\\desktop\\efficientword-net\\venv\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\salos\\onedrive\\desktop\\efficientword-net\\venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\salos\\onedrive\\desktop\\efficientword-net\\venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from audiomentations import Compose, TimeStretch, PitchShift, Shift\n",
    "import torchaudio\n",
    "import torchaudio.functional as TF\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "# --- Global Settings ---\n",
    "SAMPLE_RATE = 16000\n",
    "AUDIO_WINDOW = 1.0  # seconds\n",
    "AUDIO_LENGTH = int(AUDIO_WINDOW * SAMPLE_RATE)\n",
    "LOG_MEL_MEAN = 1.4\n",
    "LOG_MEL_STD = 1.184\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def randomCrop(x: np.array, length=AUDIO_LENGTH) -> np.array:\n",
    "    assert(x.shape[0] > length)\n",
    "    frontBits = random.randint(0, x.shape[0] - length)\n",
    "    return x[frontBits:frontBits + length]\n",
    "\n",
    "def addPadding(x: np.array, length=AUDIO_LENGTH) -> np.array:\n",
    "    assert(x.shape[0] < length)\n",
    "    bitCountToBeAdded = length - x.shape[0]\n",
    "    frontBits = random.randint(0, bitCountToBeAdded)\n",
    "    new_x = np.append(np.zeros(frontBits), x)\n",
    "    new_x = np.append(new_x, np.zeros(bitCountToBeAdded - frontBits))\n",
    "    assert new_x.shape[0] == length, f\"Error: Padded audio shape is {new_x.shape}, expected {length}\"\n",
    "    return new_x\n",
    "\n",
    "def removeExistingPadding(x: np.array) -> np.array:\n",
    "    lastZeroBitBeforeAudio = 0\n",
    "    firstZeroBitAfterAudio = len(x)\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == 0:\n",
    "            lastZeroBitBeforeAudio = i\n",
    "        else:\n",
    "            break\n",
    "    for i in range(len(x) - 1, 1, -1):\n",
    "        if x[i] == 0:\n",
    "            firstZeroBitAfterAudio = i\n",
    "        else:\n",
    "            break\n",
    "    return x[lastZeroBitBeforeAudio:firstZeroBitAfterAudio]\n",
    "\n",
    "def fixPaddingIssues(x: np.array, length=AUDIO_LENGTH) -> np.array:\n",
    "    x = removeExistingPadding(x)\n",
    "    if x.shape[0] > length:\n",
    "        return randomCrop(x, length=length)\n",
    "    elif x.shape[0] < length:\n",
    "        return addPadding(x, length=length)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def addNoise(x: np.array, noise: np.array, noise_factor=0.4) -> np.array:\n",
    "    assert(x.shape[0] == noise.shape[0])\n",
    "    out = (1 - noise_factor) * x / x.max() + noise_factor * (noise / noise.max())\n",
    "    return out / out.max()\n",
    "\n",
    "def splitNoiseFileToChunks(filename: str, target_folder: str, count=100, sr=16000):\n",
    "    noiseAudio, _ = librosa.load(filename, sr=sr)\n",
    "    if len(noiseAudio) <= AUDIO_LENGTH:\n",
    "        print(f\"Warning: Audio file {filename} is shorter than {AUDIO_LENGTH / SAMPLE_RATE} seconds. Skipping this file.\")\n",
    "        return\n",
    "\n",
    "    for i in range(count):\n",
    "        noiseAudioCrop = randomCrop(noiseAudio)\n",
    "        outFilePath = target_folder + \"/\" + (f\"{'.'.join(filename.split('.')[:-1])}_{i}.wav\").split(\"/\")[-1]\n",
    "        sf.write(outFilePath, noiseAudioCrop, sr, 'PCM_24')\n",
    "\n",
    "# --- Audio Augmentation ---\n",
    "augmentation_pipeline = Compose([\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "    Shift(min_shift=-0.5, max_shift=0.5, p=0.5),\n",
    "])\n",
    "\n",
    "# --- Mel Spectrogram Calculation (Corrected) ---\n",
    "def get_mel_spectrogram(waveform):\n",
    "    \"\"\"Calculates log-Mel spectrogram using torch.stft.\"\"\"\n",
    "    waveform = waveform.unsqueeze(1)\n",
    "\n",
    "    print(\"Waveform shape:\", waveform.shape)\n",
    "    print(\"Waveform device:\", waveform.device)\n",
    "\n",
    "    # STFT parameters\n",
    "    n_fft = 512\n",
    "    hop_length = 160\n",
    "    win_length = 512\n",
    "    window = torch.hann_window(win_length)\n",
    "\n",
    "    print(\"Window shape:\", window.shape)\n",
    "    print(\"Window device:\", window.device)\n",
    "\n",
    "    # Calculate STFT \n",
    "    stft_out = torch.stft(waveform,\n",
    "                          n_fft=n_fft,\n",
    "                          hop_length=hop_length,\n",
    "                          win_length=win_length,\n",
    "                          window=window.to(waveform.device),\n",
    "                          center=False,\n",
    "                          onesided=True,\n",
    "                          return_complex=True)\n",
    "\n",
    "    print(\"STFT output shape:\", stft_out.shape)  # <--- Print the shape \n",
    "    print(\"STFT output device:\", stft_out.device) # <--- Print the device\n",
    "\n",
    "    # Magnitude spectrogram\n",
    "    magnitude_spec = torch.abs(stft_out)\n",
    "\n",
    "\n",
    "    # Magnitude spectrogram\n",
    "    magnitude_spec = torch.abs(stft_out)\n",
    "\n",
    "    # Mel filter bank parameters\n",
    "    n_mels = 64\n",
    "    f_min = 50\n",
    "    f_max = 8000\n",
    "\n",
    "    # Create Mel filter bank\n",
    "    mel_filter_bank = TF.create_fb_matrix(\n",
    "        n_freqs=n_fft // 2 + 1,\n",
    "        f_min=f_min,\n",
    "        f_max=f_max,\n",
    "        n_mels=n_mels,\n",
    "        sample_rate=SAMPLE_RATE\n",
    "    )\n",
    "\n",
    "    # Apply filter bank\n",
    "    melspec = torch.matmul(magnitude_spec.transpose(1, 2), mel_filter_bank.to(magnitude_spec.device)).transpose(1, 2)\n",
    "\n",
    "    # Log-scaling and normalization\n",
    "    log_melspec = (torch.log(melspec + 1e-9) - LOG_MEL_MEAN) / LOG_MEL_STD\n",
    "\n",
    "    # Replicate channels for ResNet compatibility\n",
    "    log_melspec = log_melspec.repeat(1, 3, 1, 1) \n",
    "\n",
    "    return log_melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset , DataLoader\n",
    "\n",
    "class WakeWordDataset(Dataset):\n",
    "    def __init__(self, chunked_noise_path, dataset_path, transform=None,\n",
    "                 training=True, max_noise_factor=0.2, min_noise_factor=0.05,\n",
    "                 sampling_rate=16000, spectrogram=True, print_words=False):\n",
    "        super(WakeWordDataset, self).__init__()\n",
    "        self.chunked_noise_path = chunked_noise_path\n",
    "        self.dataset_path = dataset_path\n",
    "        self.transform = transform \n",
    "        self.training = training\n",
    "        self.max_noise_factor = max_noise_factor\n",
    "        self.min_noise_factor = min_noise_factor\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.spectrogram = spectrogram\n",
    "        self.print_words = print_words\n",
    "\n",
    "        self.types_of_noise = self.get_file_list(self.chunked_noise_path)\n",
    "        self.words_in_dataset = self.get_file_list(self.dataset_path)\n",
    "\n",
    "        if self.training:\n",
    "            self.words_in_dataset = self.words_in_dataset[:int(0.9 * len(self.words_in_dataset))]\n",
    "            print(\"Train size: \", len(self.words_in_dataset))\n",
    "        else:\n",
    "            self.words_in_dataset = self.words_in_dataset[int(0.9 * len(self.words_in_dataset)):]\n",
    "            print(\"Test size: \", len(self.words_in_dataset))\n",
    "\n",
    "        if self.spectrogram:\n",
    "            self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "                sample_rate=sampling_rate,\n",
    "                n_fft=512,\n",
    "                win_length=512,\n",
    "                hop_length=160,\n",
    "                n_mels=32\n",
    "            )\n",
    "\n",
    "    def get_file_list(self, path):\n",
    "        return [f for f in os.listdir(path) if not f.startswith('.')]\n",
    "\n",
    "    def give_joined_audio(self, word1, word2):\n",
    "        if self.print_words:\n",
    "            print(word1, word2)\n",
    "\n",
    "        sample1_path, sample2_path = self.get_audio_paths(word1, word2)\n",
    "        voice_vector1, _ = librosa.load(sample1_path, sr=self.sampling_rate)\n",
    "        voice_vector2, _ = librosa.load(sample2_path, sr=self.sampling_rate)\n",
    "\n",
    "        voice_vector1 = fixPaddingIssues(voice_vector1) \n",
    "        voice_vector2 = fixPaddingIssues(voice_vector2)\n",
    "\n",
    "        noise_vector1, noise_vector2 = self.load_noise()\n",
    "\n",
    "        random_noise_factor1 = random.uniform(self.min_noise_factor, self.max_noise_factor)\n",
    "        random_noise_factor2 = random.uniform(self.min_noise_factor, self.max_noise_factor)\n",
    "\n",
    "        voice_with_noise1 = addNoise(voice_vector1, noise_vector1, random_noise_factor1)\n",
    "        voice_with_noise2 = addNoise(voice_vector2, noise_vector2, random_noise_factor2)\n",
    "\n",
    "        if self.spectrogram:\n",
    "            voice_with_noise1 = torch.tensor(voice_with_noise1, dtype=torch.float32).unsqueeze(0)\n",
    "            voice_with_noise2 = torch.tensor(voice_with_noise2, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            voice_with_noise_spectrogram1 = self.mel_spectrogram(voice_with_noise1)\n",
    "            voice_with_noise_spectrogram2 = self.mel_spectrogram(voice_with_noise2)\n",
    "            \n",
    "            # Convert to decibel scale\n",
    "            voice_with_noise_spectrogram1 = torchaudio.transforms.AmplitudeToDB()(voice_with_noise_spectrogram1)\n",
    "            voice_with_noise_spectrogram2 = torchaudio.transforms.AmplitudeToDB()(voice_with_noise_spectrogram2)\n",
    "            \n",
    "            return voice_with_noise_spectrogram1.squeeze(0), voice_with_noise_spectrogram2.squeeze(0)\n",
    "        else:\n",
    "            return voice_with_noise1, voice_with_noise2\n",
    "\n",
    "    def get_audio_paths(self, word1, word2):\n",
    "        if word1 == word2:\n",
    "            sample1, sample2 = random.sample(self.get_file_list(os.path.join(self.dataset_path, word1)), 2)\n",
    "            return (os.path.join(self.dataset_path, word1, sample1),\n",
    "                    os.path.join(self.dataset_path, word2, sample2))\n",
    "        else:\n",
    "            sample1 = random.choice(self.get_file_list(os.path.join(self.dataset_path, word1)))\n",
    "            sample2 = random.choice(self.get_file_list(os.path.join(self.dataset_path, word2)))\n",
    "            return (os.path.join(self.dataset_path, word1, sample1),\n",
    "                    os.path.join(self.dataset_path, word2, sample2))\n",
    "    def load_noise(self):\n",
    "        noise_type1, noise_type2 = random.sample(self.types_of_noise, 2)\n",
    "        noise_file1 = random.choice(self.get_file_list(os.path.join(self.chunked_noise_path, noise_type1)))\n",
    "        noise_file2 = random.choice(self.get_file_list(os.path.join(self.chunked_noise_path, noise_type2)))\n",
    "\n",
    "        noise_vector1, _ = librosa.load(os.path.join(self.chunked_noise_path, noise_type1, noise_file1),\n",
    "                                        sr=self.sampling_rate)\n",
    "        noise_vector2, _ = librosa.load(os.path.join(self.chunked_noise_path, noise_type2, noise_file2),\n",
    "                                        sr=self.sampling_rate)\n",
    "        return noise_vector1, noise_vector2\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        word1 = self.words_in_dataset[index // 2] \n",
    "        word2 = self.words_in_dataset[(index // 2 + index % 2) % (len(self.words_in_dataset) // 2)]\n",
    "\n",
    "        x1, x2 = self.give_joined_audio(word1, word2)\n",
    "        \n",
    "        y = 1.0 if index % 2 == 0 else 0.0  # 1 for match, 0 for mismatch\n",
    "        return (x1, x2), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 2 * len(self.words_in_dataset) # Double for positive & negative pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseResNet, self).__init__()\n",
    "        self.resnet = torchvision.models.resnet50(pretrained=True)\n",
    "        # No need to modify conv1 \n",
    "\n",
    "        # Freeze early layers\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.resnet.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(2048, 128)\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.normalize(x, p=2, dim=1)  # L2 normalization\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(output1, output2, y_true, margin=1.0):\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2, p=2)\n",
    "    loss_contrastive = torch.mean((1 - y_true) * torch.pow(euclidean_distance, 2) +\n",
    "                                    (y_true) * torch.pow(torch.clamp(margin - euclidean_distance, min=0.0), 2))\n",
    "    return loss_contrastive\n",
    "\n",
    "def accuracy(output1, output2, y_true, threshold=0.2):\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2, p=2)\n",
    "    predictions = (euclidean_distance < threshold).float()\n",
    "    correct = (predictions == y_true).float().sum()\n",
    "    return (correct / len(y_true)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  21157\n",
      "Test size:  2351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salos\\OneDrive\\Desktop\\EfficientWord-Net\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\salos\\OneDrive\\Desktop\\EfficientWord-Net\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 32, 32, 101] to have 3 channels, but got 32 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m data1, data2, y \u001b[38;5;241m=\u001b[39m data1\u001b[38;5;241m.\u001b[39mto(device), data2\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 26\u001b[0m output1, output2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata2\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m triplet_loss(output1, output2, y)\n\u001b[0;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\salos\\OneDrive\\Desktop\\EfficientWord-Net\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\salos\\OneDrive\\Desktop\\EfficientWord-Net\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m, in \u001b[0;36mSiameseResNet.forward\u001b[1;34m(self, input1, input2)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input1, input2):\n\u001b[1;32m---> 25\u001b[0m     output1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     output2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_one(input2)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output1, output2\n",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m, in \u001b[0;36mSiameseResNet.forward_one\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_avg_pool(x)\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\salos\\OneDrive\\Desktop\\EfficientWord-Net\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\salos\\OneDrive\\Desktop\\EfficientWord-Net\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\salos\\OneDrive\\Desktop\\EfficientWord-Net\\venv\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\salos\\OneDrive\\Desktop\\EfficientWord-Net\\venv\\lib\\site-packages\\torchvision\\models\\resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m    270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32mc:\\Users\\salos\\OneDrive\\Desktop\\EfficientWord-Net\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\salos\\OneDrive\\Desktop\\EfficientWord-Net\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\salos\\OneDrive\\Desktop\\EfficientWord-Net\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\salos\\OneDrive\\Desktop\\EfficientWord-Net\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 32, 32, 101] to have 3 channels, but got 32 channels instead"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 10\n",
    "    LEARNING_RATE = 1e-4\n",
    "\n",
    "    chunkedNoisePath = r\"C:\\Users\\salos\\OneDrive\\Desktop\\EfficientWord-Net\\Efficient_word_net\\NoiseChunked\" \n",
    "    datasetPath = r\"C:\\Users\\salos\\OneDrive\\Desktop\\EfficientWord-Net\\Efficient_word_net\\test\"\n",
    "\n",
    "    train_dataset = WakeWordDataset(chunkedNoisePath, datasetPath, spectrogram=True, training=True)\n",
    "    test_dataset = WakeWordDataset(chunkedNoisePath, datasetPath, spectrogram=True, training=False)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = SiameseResNet().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    writer = SummaryWriter() \n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for batch_idx, ((data1, data2), y) in enumerate(train_loader):\n",
    "            data1, data2, y = data1.to(device), data2.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output1, output2 = model(data1, data2) \n",
    "            \n",
    "            loss = triplet_loss(output1, output2, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
